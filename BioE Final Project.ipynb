{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zo2N7mTr9IBz"},"outputs":[],"source":["# This project focuses on brain CT image segmentation and denoising using Deep Learning.\n","# Al Smith and Will Newman\n","# c 2024\n","\n","# Importing the necessary libraries:\n","import pydicom\n","from collections import defaultdict\n","import numpy as np\n","import random\n","import os\n","import gc\n","from glob import glob\n","import nibabel as nib\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","from ipywidgets import widgets, interact, IntSlider\n","from scipy.ndimage import label\n","from mayavi import mlab\n","from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n","from skimage import measure\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from sklearn.model_selection import train_test_split\n","from scipy.ndimage import zoom\n","from scipy.ndimage import gaussian_filter\n"]},{"cell_type":"markdown","metadata":{},"source":["## Tasks:\n","1) Load data from CQ500\n","    * Download from: http://headctstudy.qure.ai/dataset\n","    * Explore the DICOM Header for voxel size and imaging information (ideally the CT machine model)\n","    * Resolution, dose, parameters, etc.\n","2) Preprocess the data (noise addition, downsampling, agumentation, etc)\n","    * 2a) Add noise similar to low-resolution CT\n","    * 2b) Downsample the images to lower-resolution scale\n","    * 2c) Split data into train/test sets\n","3) Build a 3D U-Net model for segmentation\n","    * 3a) start with training the model on CQ500 and normal masks\n","    * 3b) train the model on CQ500 with noise added\n","    * 3c) train the model on CQ500 with images processed by denoising model\n","4) Denoising Model for noisy CT images\n","5) Train the model\n","6) Evaluate the model"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Global Variables\n","lb = 1040\n","ub = 1080"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def save_nifti(mask, output_dir, series_uid):\n","    \"\"\"Save the mask as a NIfTI file.\"\"\"\n","    nifti_image = nib.Nifti1Image(mask.astype(np.int16), affine=np.eye(4))\n","    nib.save(nifti_image, os.path.join(output_dir, f'{series_uid}_mask.nii'))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Apply window and level to a 2D numpy array of DICOM data\n","def apply_window_level(data, lb, ub):\n","    windowed_data = np.clip(data, lb, ub)\n","    normalized_data = (windowed_data - lb) / (ub - lb)  # Normalize between 0 and 1\n","    return normalized_data\n","\n","# Remove unnecessary CSF spaces from the mask\n","def remove_mask_below_slice(volume_mask, slice_index):\n","    \"\"\"\n","    Set the mask to False for all slices below the specified index.\n","\n","    Parameters:\n","    - volume_mask: 3D numpy array (boolean) of the mask.\n","    - slice_index: Integer, the index below which the mask should be removed.\n","    \"\"\"\n","    # Set all slices below the specified index to False\n","    volume_mask[:slice_index, :, :] = False\n","    return volume_mask\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Largest connected component segmentation\n","def create_spherical_mask(shape, center, radius):\n","    z, y, x = np.ogrid[:shape[0], :shape[1], :shape[2]]\n","    dist_from_center = (z - center[0])**2 + (y - center[1])**2 + (x - center[2])**2\n","    return dist_from_center <= radius**2\n","\n","\n","def largest_connected_component_3d(volume_data, lb, center, radius, threshold=0.4):\n","    \"\"\"\n","    Find the largest connected component in a 3D volume that intersects with a specified spherical region.\n","\n","    Parameters:\n","    - volume_data: 3D numpy array of DICOM data.\n","    - lb: Lower bound to create a binary mask for values of interest.\n","    - center: Tuple, the center coordinates (z, y, x) of the volume.\n","    - radius: Integer, the radius used to define the spherical region.\n","\n","    Returns:\n","    - 3D mask (boolean array) of the same shape as volume_data for the largest component intersecting the sphere.\n","    \"\"\"\n","    # Generate the binary mask\n","    slice_index = int(volume_data.shape[0] * 0.4)\n","    binary_mask = volume_data < lb\n","    binary_mask = remove_mask_below_slice(binary_mask, slice_index)\n","\n","    # Label all components\n","    labeled_volume, num_features = label(binary_mask)\n","    if num_features == 0:\n","        return np.zeros_like(volume_data, dtype=bool)  # No components found\n","\n","    # Generate the spherical mask\n","    spherical_mask = create_spherical_mask(volume_data.shape, center, radius)\n","\n","    # Find labels intersecting the spherical mask\n","    intersecting_labels = np.unique(labeled_volume[spherical_mask])\n","\n","    # Calculate the size of each intersecting component and select the largest\n","    largest_label = None\n","    max_size = 0\n","    for label_idx in intersecting_labels:\n","        if label_idx == 0:\n","            continue  # Skip background\n","        component_mask = labeled_volume == label_idx\n","        component_size = np.sum(component_mask)\n","        if component_size > max_size:\n","            max_size = component_size\n","            largest_label = label_idx\n","\n","    return labeled_volume == largest_label if largest_label is not None else np.zeros_like(volume_data, dtype=bool)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved mask to  /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Saved mask to  /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Saved mask to  /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Saved mask to  /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Saved mask to  /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Processed 987 DICOM files.\n","Number of series with loaded volumes: 5\n"]}],"source":["def load_dicom_series_volumes(base_directory, idx):\n","    series_volumes = defaultdict(list)\n","    # Track the number of DICOM files processed\n","    dicom_file_count = 0\n","    \n","    # Construct the directory path\n","    case_dir = os.path.join(base_directory, f\"CQ500CT{idx} CQ500CT{idx}\", \"Unknown Study\")\n","    \n","    if os.path.exists(case_dir):\n","        # Walk through all files in the series directories within the \"Unknown Study\" directory\n","        for root, dirs, files in os.walk(case_dir):\n","            for dir in dirs:\n","                series_path = os.path.join(root, dir)\n","                slices = []\n","                # Collect all DICOM slices in the series directory\n","                for slice_file in os.listdir(series_path):\n","                    if slice_file.lower().endswith('.dcm'):\n","                        full_path = os.path.join(series_path, slice_file)\n","                        try:\n","                            # Read the DICOM file\n","                            dicom_slice = pydicom.dcmread(full_path)\n","                            # Append the slice and its position for later sorting\n","                            slices.append((dicom_slice, dicom_slice.ImagePositionPatient[2]))\n","                            dicom_file_count += 1\n","                        except Exception as e:\n","                            print(f\"Failed to read {slice_file} as DICOM: {e}\")\n","\n","                # Sort slices based on the z-coordinate (ImagePositionPatient[2])\n","                slices.sort(key=lambda x: x[1])\n","                # Stack the pixel data from sorted slices to form a 3D volume\n","                if slices:\n","                    series_uid = slices[0][0].SeriesInstanceUID\n","                    volume = np.stack([s[0].pixel_array for s in slices])\n","                    series_volumes[series_uid].append(volume)\n","\n","                    center = (volume.shape[0] // 2, volume.shape[1] // 2, volume.shape[2] // 2)\n","                    radius = int(volume.shape[0] // 6)  # Define the radius as desired\n","\n","                    # Generate mask and save\n","                    mask = remove_mask_below_slice(largest_connected_component_3d(volume, lb, center, radius), volume.shape[0] * 2 // 5)\n","                    save_nifti(mask, case_dir, series_uid)\n","                    print(\"Saved mask to \", case_dir)\n","    else:\n","        print(f\"Directory does not exist: {case_dir}\")\n","\n","    print(f\"Processed {dicom_file_count} DICOM files.\")\n","    return series_volumes\n","\n","# Usage example\n","base_directory = '/media/hal9000/Database/CQ500_extracted'  # Adjust this path\n","dicom_volumes = load_dicom_series_volumes(base_directory, 0)\n","print(f\"Number of series with loaded volumes: {len(dicom_volumes)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# DICOM Directories that are useful\n","* Series Number    0, 3, 4, 5, 7, 9, 14, 15, 19, 23, 26, 28, 33, 36, 37, 39, 40, 41, 43, 45, 46, 50, 56, 58, 59, 62, 63, 64, 70, 71, 76, 78, 79, 81, 82, 83, 84, 85, 89, 91, 92, 99, 100, 101, 102, 103, 104, 105, 106, 109, 113, 116, 117\n","* Number of Masks: 5, 2, 1, 1, 1, 2,  2,  1,  4,  2,  3,  2,  2,  3,  2,  2,  2,  1,  1,  1,  1,  1,  2,  3,  1,  2,  2,  3,  1,  2,  2, 1,   1,   1,  1,  1,  1,  3,  3,  1,  4,  1,   2,   3,   1,   2,   1,   3,   4,   2,   2,   1,   3\n","\n","# DICOM Directories with surgical pathology (less useful)\n","* Series Number    2, 6, 10, 11, 17, 18, 20, 22, 34, 48, 55, 57, 60, 64, 66, 68, 86, 90, 108, 111, 114,\n","* Number of Masks: 2, 3,  1,  2,  2,  2,  2,  2,  3,  4,  2,  2,  4,  1,  5,  1,  2,  2,   2,   2,   1,"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65234ccbffb445a4b9d8d5fbf77d63a3","version_major":2,"version_minor":0},"text/plain":["interactive(children=(IntSlider(value=0, description='Slice Index:', max=238), Output()), _dom_classes=('widge…"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["# Visualize Loaded DICOM Data\n","\n","# Assume dicom_volumes is already loaded\n","series_selection =  1\n","dicom =             0\n","series_uid = list(dicom_volumes.keys())[series_selection]\n","print(series_uid)\n","volume = dicom_volumes[series_uid][0]  # Get the first volume of the first series\n","\n","center = (volume.shape[0] // 2, volume.shape[1] // 2, volume.shape[2] // 2)\n","radius = int(volume.shape[0] // 6)  # Define the radius as desired\n","#volume_mask = remove_mask_below_slice(largest_connected_component_3d(volume, lb, center, radius), volume.shape[0] * 2 // 5)\n","volume_mask = nib.load(f'/media/hal9000/Database/CQ500_extracted/CQ500CT{dicom} CQ500CT{dicom}/Unknown Study/{series_uid}_mask.nii').get_fdata() > 0\n","\n","# Function to display a single slice\n","def view_slice(slice_index):\n","    plt.figure(figsize=(4, 4))\n","    processed_image = apply_window_level(volume[slice_index], lb, ub)\n","    \n","    # Overlay the 3D mask on the corresponding slice\n","    overlay = np.zeros(processed_image.shape + (4,))  # RGBA\n","    overlay[..., 0] = 1.0  # Red channel\n","    overlay[..., 3] = volume_mask[slice_index] * 0.5  # Semi-transparent where the mask is True\n","\n","    plt.imshow(processed_image, cmap='gray')\n","    plt.imshow(overlay)\n","    plt.axis('off')\n","    plt.title(f'Slice {slice_index + 1}')\n","    plt.show()\n","\n","# Slider to select the slice index\n","slice_slider = widgets.IntSlider(\n","    value=0,\n","    min=0,\n","    max=volume.shape[0] - 1,  # max slice index\n","    step=1,\n","    description='Slice Index:',\n","    continuous_update=True\n",")\n","\n","# Use ipywidgets' interactive functionality to bind the slider and the display function\n","widgets.interactive(view_slice, slice_index=slice_slider)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Visualize the CSF Space Segmentation Mask in 3D dynamic viewer\n","def visualize_3d_mask(volume_mask):\n","    \"\"\"Visualize a 3D mask using mayavi's volume rendering capabilities.\"\"\"\n","    # Create a figure\n","    fig = mlab.figure(bgcolor=(0, 0, 0), size=(800, 800))\n","    \n","    # Visualize the volume mask: 1s are turned to True, 0s to False\n","    src = mlab.pipeline.scalar_field(zoom(volume_mask.astype(int), (4, 1, 1)))\n","    # Threshold to visualize only the 1s\n","    mlab.pipeline.iso_surface(src, contours=[volume_mask.min()+0.5, volume_mask.max()], opacity=0.4, color=(1, 0, 0))\n","    \n","    # Enhance the view\n","    mlab.view(azimuth=180, elevation=180, distance=400)\n","    mlab.roll(180)\n","    \n","    # Add axes and outline for better visual orientation\n","    mlab.outline(src, color=(1, 1, 1))\n","    mlab.axes(src, color=(1, 1, 1), xlabel='X', ylabel='Y', zlabel='Z')\n","\n","    # Show the plot\n","    mlab.show()\n","\n","# Assuming volume_mask is the mask calculated earlier\n","visualize_3d_mask(volume_mask)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n"]}],"source":["# SAVE SLICES AS IMAGES (convert to GIF online)\n","# Assume dicom_volumes is already loaded\n","series_selection =  3\n","dicom =             0\n","series_uid = list(dicom_volumes.keys())[series_selection]\n","print(series_uid)\n","volume = dicom_volumes[series_uid][0]  # Get the first volume of the first series\n","\n","# Create a directory to save the images\n","output_dir = './output_images'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Function to save a single slice as an image\n","def save_slice(slice_index):\n","    plt.figure(figsize=(4, 4))\n","    processed_image = apply_window_level(volume[slice_index], lb, ub)\n","    \n","    # Overlay the 3D mask on the corresponding slice\n","    overlay = np.zeros(processed_image.shape + (4,))  # RGBA\n","    overlay[..., 0] = 1.0  # Red channel\n","    overlay[..., 3] = volume_mask[slice_index] * 0.5  # Semi-transparent where the mask is True\n","\n","    plt.imshow(processed_image, cmap='gray')\n","    plt.imshow(overlay)\n","    plt.axis('off')\n","    plt.title(f'Slice {slice_index + 1}')\n","    \n","    # Save the figure\n","    plt.savefig(os.path.join(output_dir, f'slice_{slice_index + 1}.png'))\n","    plt.close()\n","\n","# Save all slices as images\n","for slice_index in range(volume.shape[0]):\n","    save_slice(slice_index)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714313.2033589\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT3 CQ500CT3/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713849.1919689\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713848.1919208\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT4 CQ500CT4/Unknown Study\n"]},{"name":"stderr","output_type":"stream","text":["/home/hal9000/.local/lib/python3.10/site-packages/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (16-bit) doesn't match the JPEG 2000 data (14-bit). It's recommended that you change the 'Bits Stored' value\n","  warnings.warn(\n","/home/hal9000/.local/lib/python3.10/site-packages/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (16-bit) doesn't match the JPEG 2000 data (13-bit). It's recommended that you change the 'Bits Stored' value\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713544.1838788\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT5 CQ500CT5/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713672.1872547\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT7 CQ500CT7/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713385.1794631\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT9 CQ500CT9/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713737.1889019\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713734.1888437\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT14 CQ500CT14/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714487.2066237\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714484.2065631\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT15 CQ500CT15/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713469.1816824\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT19 CQ500CT19/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714080.1983083\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714081.1983544\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714084.1984457\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714083.1984005\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT23 CQ500CT23/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713168.1741418\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713170.1741990\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT26 CQ500CT26/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713354.1788238\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713356.1788780\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713355.1788299\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT28 CQ500CT28/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713970.1954646\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713968.1954066\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT33 CQ500CT33/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714287.2028344\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714288.2028866\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT36 CQ500CT36/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713135.1733306\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713135.1733367\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713135.1733431\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT37 CQ500CT37/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713314.1778626\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713312.1778142\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT39 CQ500CT39/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713254.1764192\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713254.1764256\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT40 CQ500CT40/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714581.2081670\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714580.2081597\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT41 CQ500CT41/Unknown Study\n"]},{"name":"stderr","output_type":"stream","text":["/home/hal9000/.local/lib/python3.10/site-packages/pydicom/pixel_data_handlers/pillow_handler.py:238: UserWarning: The (0028,0101) 'Bits Stored' value (12-bit) doesn't match the JPEG 2000 data (11-bit). It's recommended that you change the 'Bits Stored' value\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713857.1921895\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT43 CQ500CT43/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714173.2003120\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT45 CQ500CT45/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713842.1917402\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT46 CQ500CT46/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714540.2075073\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT50 CQ500CT50/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713549.1840468\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT56 CQ500CT56/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714233.2015756\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714236.2016337\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT58 CQ500CT58/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714698.2100883\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714698.2100813\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714695.2100294\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT59 CQ500CT59/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714904.2139255\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT62 CQ500CT62/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713760.1895252\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713759.1894734\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT63 CQ500CT63/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714672.2096600\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714669.2096095\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT64 CQ500CT64/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714473.2063803\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT70 CQ500CT70/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713302.1775602\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT71 CQ500CT71/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713176.1743714\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713178.1744306\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT76 CQ500CT76/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713787.1902086\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713785.1901506\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT78 CQ500CT78/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713504.1827067\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT79 CQ500CT79/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713879.1928508\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT81 CQ500CT81/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521712980.1693800\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT82 CQ500CT82/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713921.1940853\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT83 CQ500CT83/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713358.1788997\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT84 CQ500CT84/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714125.1994075\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT85 CQ500CT85/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713931.1943819\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713929.1943241\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713931.1943754\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT89 CQ500CT89/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713119.1729036\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713119.1729097\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713119.1729158\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT91 CQ500CT91/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713310.1777831\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT92 CQ500CT92/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713328.1781845\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713325.1781329\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713325.1781264\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713324.1780869\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT99 CQ500CT99/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713279.1770817\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT100 CQ500CT100/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713659.1870254\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713655.1869579\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT101 CQ500CT101/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713389.1795354\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713387.1794905\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713386.1794845\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT102 CQ500CT102/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714399.2051535\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT103 CQ500CT103/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713040.1709447\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713038.1708934\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT104 CQ500CT104/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713344.1785724\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT105 CQ500CT105/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713205.1751675\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713208.1752197\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713208.1752259\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT55 CQ500CT55/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713584.1850772\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713586.1851352\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT109 CQ500CT109/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713526.1833521\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713528.1834034\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT113 CQ500CT113/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714133.1996365\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714131.1995820\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT116 CQ500CT116/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521713329.1781999\n","Searching in: /media/hal9000/Database/CQ500_extracted/CQ500CT117 CQ500CT117/Unknown Study\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714863.2132400\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714862.2132336\n","Mask found and loaded for series UID 1.2.276.0.7230010.3.1.3.296485376.1.1521714860.2131866\n","Loaded 98 series with both images and masks.\n"]}],"source":["# Task #1 Convert loaded data into torch training and validation datasets\n","def load_dicom_series_volumes(base_directory, indices, target_size=(32, 128, 128)):\n","    series_data = defaultdict(lambda: {'images': None, 'mask': None})\n","\n","    for idx in indices:\n","        case_dir = os.path.join(base_directory, f\"CQ500CT{idx} CQ500CT{idx}\", \"Unknown Study\")\n","        print(f\"Searching in: {case_dir}\")\n","\n","        if os.path.exists(case_dir):\n","            mask_files = {os.path.basename(f).split('_mask.nii')[0]: f for f in glob(os.path.join(case_dir, '*_mask.nii'))}\n","\n","            for subdir in os.listdir(case_dir):\n","                subdir_path = os.path.join(case_dir, subdir)\n","                if not os.path.isdir(subdir_path):\n","                    continue\n","\n","                slices = []\n","                series_uid = None\n","                for slice_file in glob(os.path.join(subdir_path, '*.dcm')):\n","                    try:\n","                        dicom_slice = pydicom.dcmread(slice_file)\n","                        slices.append(dicom_slice)\n","                        series_uid = dicom_slice.SeriesInstanceUID\n","                    except Exception as e:\n","                        print(f\"Failed to read {slice_file} as DICOM: {e}\")\n","\n","                if slices and series_uid:\n","                    slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n","                    volume = np.stack([s.pixel_array for s in slices])\n","\n","                    # Resize the volume\n","                    resize_factor = np.array(target_size) / np.array(volume.shape)\n","                    resized_volume = zoom(volume, resize_factor)\n","                    series_data[series_uid]['images'] = resized_volume\n","\n","                    if series_uid in mask_files:\n","                        mask_file = mask_files[series_uid]\n","                        mask = nib.load(mask_file).get_fdata()\n","\n","                        # Resize the mask\n","                        resized_mask = zoom(mask, resize_factor)\n","                        resized_mask = (resized_mask > 0.5).astype(np.float32)\n","                        series_data[series_uid]['mask'] = resized_mask\n","                        print(f\"Mask found and loaded for series UID {series_uid}\")\n","        else:\n","            print(f\"No directory found for index {idx}\")\n","\n","    filtered_data = {uid: data for uid, data in series_data.items() if data['images'] is not None and data['mask'] is not None}\n","    return filtered_data\n","\n","# Usage example\n","base_directory = '/media/hal9000/Database/CQ500_extracted/'\n","# all indices from \"good scans\", where first comment is 10 scans, second is 64, and third is 100, spaces indicate where to comment out\n","indices = [0, 3, 4, 5, 7, \\\n","           9, 14, 15, 19, 23, 26, 28, 33, 36, 37, 39, 40, 41, 43, 45, 46, 50, 56, 58, 59, 62, 63, \\\n","           64, 70, 71, 76, 78, 79, 81, 82, 83, 84, \\\n","           85, 89, 91, 92, 99, 100, 101, 102, 103, 104, 105, 55, 109, 113, 116, 117]\n","loaded_data = load_dicom_series_volumes(base_directory, indices)\n","print(f\"Loaded {len(loaded_data)} series with both images and masks.\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.2.276.0.7230010.3.1.3.296485376.1.1521714084.1984457\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c286ea6e7c74c029d89528b2f50a06c","version_major":2,"version_minor":0},"text/plain":["interactive(children=(IntSlider(value=16, description='slice_index', max=31), Output()), _dom_classes=('widget…"]},"metadata":{},"output_type":"display_data"}],"source":["# Make sure images are loaded correctly\n","\n","def visualize_images_and_masks(image_volume, mask_volume):\n","    \"\"\"\n","    Visualize DICOM images with their corresponding masks using matplotlib and ipywidgets.\n","\n","    Parameters:\n","    - image_volume: 3D numpy array of the DICOM images.\n","    - mask_volume: 3D numpy array of the mask data.\n","    \"\"\"\n","\n","    image_volume = np.clip(image_volume, lb, ub)  # Apply window and level\n","\n","    # Create a colormap that includes transparency\n","    colors = [(0,0,0,0)] + [(plt.cm.Reds(i)) for i in range(1,256)]\n","    RedAlpha = mcolors.LinearSegmentedColormap.from_list('RedAlpha', colors, N=256)\n","\n","    # Function to update the plot for each slice\n","    def plot_slice(slice_index):\n","        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n","        \n","        # Show the mask slice\n","        ax.imshow(image_volume[slice_index], cmap='gray')\n","        ax.imshow(mask_volume[slice_index], alpha=0.75, cmap=RedAlpha)  # Overlay mask\n","        ax.set_title('Image with Mask')\n","        ax.axis('off')\n","\n","        plt.show()\n","\n","    # Create a slider to scroll through slices\n","    interact(plot_slice, slice_index=IntSlider(min=0, max=image_volume.shape[0] - 1, step=1, value=image_volume.shape[0] // 2))\n","\n","# Example usage with the first loaded series (assuming loaded_data is available)\n","series_uid = random.choice(list(loaded_data.keys()))    # Randomly select a series\n","print(series_uid)\n","visualize_images_and_masks(loaded_data[series_uid]['images'], loaded_data[series_uid]['mask'])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["class CTScanDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        uid, data = list(self.data.items())[idx]\n","        image = np.array(data['images'])  # Convert memmap to regular numpy array\n","        mask = np.array(data['mask'])  # Convert memmap to regular numpy array\n","        return image, mask"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Define a custom collate function\n","\n","batch_size = 6\n","\n","def collate_fn(batch):\n","    images, masks = zip(*batch)\n","    \n","    # Convert the numpy arrays to PyTorch tensors\n","    images = [torch.from_numpy(image.astype(np.float32)) for image in images]\n","    masks = [torch.from_numpy(mask.astype(np.uint8)) for mask in masks]\n","    \n","    # Determine the maximum size along the 0 axis\n","    max_size = max(image.shape[0] for image in images)\n","    \n","    # Pad the images and masks to the maximum size\n","    images = [F.pad(image.float(), (0, 0, 0, 0, 0, max_size - image.shape[0])) for image in images]\n","    masks = [F.pad(mask.float(), (0, 0, 0, 0, 0, max_size - mask.shape[0])) for mask in masks]\n","    \n","    return torch.stack(images), torch.stack(masks)\n","\n","# Data Split\n","def create_train_test_split(data_dict, test_size=0.25, random_state=42):\n","    # Create a list of keys, which are the series UIDs\n","    series_uids = list(data_dict.keys())\n","    \n","    # Split the series UIDs into train and test sets\n","    train_uids, test_uids = train_test_split(series_uids, test_size=test_size, random_state=random_state)\n","    \n","    # Create subsets for train and test from the main dataset\n","    train_dataset = {uid: data_dict[uid] for uid in train_uids}\n","    test_dataset = {uid: data_dict[uid] for uid in test_uids}\n","    \n","    return train_dataset, test_dataset\n","\n","# Assume 'loaded_data' is your fully loaded dataset dictionary\n","train_data, test_data = create_train_test_split(loaded_data, test_size=0.4)\n","\n","# Assuming a transform defined earlier\n","train_dataset = CTScanDataset(train_data)\n","test_dataset = CTScanDataset(test_data)\n","\n","# Create DataLoaders for both train and test datasets\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Task #2 Preprocess the data\n","# Task #2b Add noise similar to low-resolution CT\n","\n","# Task #2c Downsample the images to lower-resolution scale\n","\n","# Task #2d Split the data into training and testing sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Task #3 Build a 3D U-Net model for segmentation\n","# Task #3a Start with training the model on CQ500 and normal masks\n","\n","# Task #3b Train the model on CQ500 with noise added\n","\n","# Task #3c Train the model on CQ500 with images processed by denoising model"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# Task 3 Using nnU-Net architecture as described in the paper: https://arxiv.org/abs/2404.09556\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv3d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm3d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv3d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm3d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool3d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","        \n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose3d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","            \n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffZ = torch.tensor([x2.size()[2] - x1.size()[2]])\n","        diffY = torch.tensor([x2.size()[3] - x1.size()[3]])\n","        diffX = torch.tensor([x2.size()[4] - x1.size()[4]])\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2,\n","                        diffZ // 2, diffZ - diffZ // 2])\n","        # if you have padding issues, see below\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNet3D(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=True):\n","        super(UNet3D, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = Down(64, 128)\n","        self.down2 = Down(128, 256)\n","        self.down3 = Down(256, 512)\n","        self.up1 = Up(768, 256, bilinear)\n","        self.up2 = Up(384, 128, bilinear)\n","        self.up3 = Up(192, 64, bilinear)\n","        self.outc = OutConv(64, n_classes)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x = self.up1(x4, x3)\n","        x = self.up2(x, x2)\n","        x = self.up3(x, x1)\n","        logits = self.outc(x)\n","        return logits"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# Use Sørensen-Dice coefficient as the loss function\n","class DiceLoss(nn.Module):\n","    def __init__(self, smooth=1e-6):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, inputs, targets):\n","        # Convert probabilities to binary predictions\n","        # For a multi-class problem, consider each class channel as a binary problem\n","        inputs = torch.sigmoid(inputs)  # Apply sigmoid to squash outputs to [0,1]\n","        \n","        # Flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","        \n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n","        \n","        return 1 - dice"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device: \", device)\n","model = UNet3D(n_channels=1, n_classes=1).to(device)\n","#criterion = DiceLoss()\n","criterion = DiceLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","num_epochs = 200\n","loss = 0.0\n","loss_history = []\n","val_loss_history = []\n","def to_grayscale(images):\n","    return images.mean(dim=1, keepdim=True).squeeze(1)"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[],"source":["# garbace collection to prevent gpu memory overflow\n","gc.collect()\n","model = None\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Progress: 100%|██████████| 125/125 [45:01<00:00, 21.62s/it, Epoch=125, Avg Loss=0.369, Avg Val Loss=0.435]\n"]}],"source":["# Training Loop\n","squeeze_factor = 1\n","num_epochs = 125 # Set this to your desired number of epochs\n","\n","progress_bar_epochs = tqdm(range(num_epochs), total=num_epochs, desc=\"Training Progress\")\n","\n","for epoch in progress_bar_epochs:\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    running_val_loss = 0.0\n","\n","    for i, (images, masks) in enumerate(train_loader):\n","        images = images.float().to(device)\n","        masks = masks.float().to(device)\n","\n","        images = images.unsqueeze(squeeze_factor)\n","        masks = masks.unsqueeze(squeeze_factor)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        outputs = torch.nn.functional.interpolate(outputs, size=images.size()[2:])\n","        loss = criterion(outputs, masks)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        del images, masks, outputs\n","        torch.cuda.empty_cache()\n","\n","    # Calculate the average loss for the epoch\n","    average_loss = running_loss / len(train_loader)\n","    loss_history.append(average_loss)\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        running_val_loss = 0.0\n","        for i, (images, masks) in enumerate(test_loader):\n","            images = images.float().to(device)\n","            masks = masks.float().to(device)\n","            \n","            images = images.unsqueeze(squeeze_factor)\n","            masks = masks.unsqueeze(squeeze_factor)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            val_loss = criterion(outputs, masks)\n","\n","            running_val_loss += val_loss.item()\n","\n","        # Calculate the average validation loss for the epoch\n","        average_val_loss = running_val_loss / len(test_loader)\n","        val_loss_history.append(average_val_loss)\n","\n","    # Print losses every epoch\n","    progress_bar_epochs.set_postfix({'Epoch': epoch+1, 'Avg Loss': average_loss, 'Avg Val Loss': average_val_loss})"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Progress: 100%|██████████| 10/10 [03:44<00:00, 22.41s/it, Epoch=10, Avg Loss=0.357, Avg Val Loss=0.381]\n"]}],"source":["# Training Loop\n","squeeze_factor = 1\n","num_epochs = 10 # Set this to your desired number of epochs\n","\n","progress_bar_epochs = tqdm(range(num_epochs), total=num_epochs, desc=\"Training Progress\")\n","\n","for epoch in progress_bar_epochs:\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    running_val_loss = 0.0\n","\n","    for i, (images, masks) in enumerate(train_loader):\n","        images = images.float().to(device)\n","        masks = masks.float().to(device)\n","\n","        images = images.unsqueeze(squeeze_factor)\n","        masks = masks.unsqueeze(squeeze_factor)\n","\n","        # Clip the images to the lower and upper bounds\n","        images = torch.clamp(images, lb, ub)\n","\n","        # Normalize the images to the range [0, 1]\n","        images = (images - lb) / (ub - lb)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        outputs = torch.nn.functional.interpolate(outputs, size=images.size()[2:])\n","        loss = criterion(outputs, masks)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        del images, masks, outputs\n","        torch.cuda.empty_cache()\n","\n","    # Calculate the average loss for the epoch\n","    average_loss = running_loss / len(train_loader)\n","    loss_history.append(average_loss)\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        running_val_loss = 0.0\n","        for i, (images, masks) in enumerate(test_loader):\n","            images = images.float().to(device)\n","            masks = masks.float().to(device)\n","            \n","            images = images.unsqueeze(squeeze_factor)\n","            masks = masks.unsqueeze(squeeze_factor)\n","\n","            # Clip the images to the lower and upper bounds\n","            images = torch.clamp(images, lb, ub)\n","\n","            # Normalize the images to the range [0, 1]\n","            images = (images - lb) / (ub - lb)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            val_loss = criterion(outputs, masks)\n","\n","            running_val_loss += val_loss.item()\n","\n","        # Calculate the average validation loss for the epoch\n","        average_val_loss = running_val_loss / len(test_loader)\n","        val_loss_history.append(average_val_loss)\n","\n","    # Print losses every epoch\n","    progress_bar_epochs.set_postfix({'Epoch': epoch+1, 'Avg Loss': average_loss, 'Avg Val Loss': average_val_loss})"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Plotting the loss history and validation loss history\n","plt.plot(loss_history, label='Training Loss')\n","plt.plot(val_loss_history, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Loss over Time')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":226,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"300908162a14401aaf5f8aaa11909855","version_major":2,"version_minor":0},"text/plain":["interactive(children=(IntSlider(value=16, description='slice_index', max=31), Output()), _dom_classes=('widget…"]},"metadata":{},"output_type":"display_data"}],"source":["# Make sure images are loaded correctly\n","\n","def visualize_images_and_masks(image_volume, mask_volume, pred_mask_volume):\n","    \"\"\"\n","    Visualize DICOM images with their corresponding masks using matplotlib and ipywidgets.\n","\n","    Parameters:\n","    - image_volume: 3D numpy array of the DICOM images.\n","    - mask_volume: 3D numpy array of the mask data.\n","    \"\"\"\n","\n","    image_volume = np.clip(image_volume, lb, ub)\n","\n","    # Create a colormap that includes transparency\n","    colors = [(0,0,0,0)] + [(plt.cm.Reds(i)) for i in range(1,256)]\n","    RedAlpha = mcolors.LinearSegmentedColormap.from_list('RedAlpha', colors, N=256)\n","\n","    colors = [(0,0,0,0)] + [(plt.cm.Blues(i)) for i in range(1,256)]\n","    BlueAlpha = mcolors.LinearSegmentedColormap.from_list('BlueAlpha', colors, N=256)\n","\n","    # Function to update the plot for each slice\n","    def plot_slice(slice_index):\n","        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n","        \n","        # Show the ground truth mask slice\n","        axs[0].imshow(image_volume[slice_index], cmap='gray')\n","        axs[0].imshow(mask_volume[slice_index], alpha=0.75, cmap=RedAlpha)  # Overlay mask\n","        axs[0].set_title('Image with Ground Truth Mask')\n","        axs[0].axis('off')\n","\n","        # Show the predicted mask slice\n","        axs[1].imshow(image_volume[slice_index], cmap='gray')\n","        axs[1].imshow(pred_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[1].set_title('Image with Predicted Mask')\n","        axs[1].axis('off')\n","\n","        plt.show()\n","\n","    # Create a slider to scroll through slices\n","    interact(plot_slice, slice_index=IntSlider(min=0, max=image_volume.shape[0] - 1, step=1, value=image_volume.shape[0] // 2))\n","\n","series_uid = random.choice(list(loaded_data.keys()))    # Randomly select a series\n","\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():  # Disable gradient calculation\n","    images = torch.from_numpy(loaded_data[series_uid]['images'].astype(np.float32)).clip(lb, ub)\n","    # Normalize the images to the range [0, 1]\n","    images = (images - lb) / (ub - lb)\n","    # Add a channel dimension (assuming grayscale images)\n","    images = images.unsqueeze(0)\n","    # Add a batch dimension\n","    images = images.unsqueeze(0)\n","    # Ensure the tensor is on the correct device\n","    images = images.to(device)\n","    outputs = model(images)  # Get the model's predictions\n","\n","# The outputs are likely in the form of logits or probabilities. \n","# You might need to apply a threshold or take the argmax to get the final predicted mask.\n","# This depends on how your model was trained.\n","\n","pred_mask_volume = (outputs > 1).float().squeeze(0)[0].cpu().numpy()  # Example for a multi-class segmentation model\n","\n","visualize_images_and_masks(loaded_data[series_uid]['images'], loaded_data[series_uid]['mask'], pred_mask_volume)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 324 DICOM files.\n","Number of series with loaded volumes: 2\n"]}],"source":["def load_dicom_series_volumes(base_directory, idx, target_size=(32, 128, 128)):\n","    series_volumes = defaultdict(list)\n","    # Track the number of DICOM files processed\n","    dicom_file_count = 0\n","    \n","    # Construct the directory path\n","    case_dir = os.path.join(base_directory, f\"CQ500CT{idx} CQ500CT{idx}\", \"Unknown Study\")\n","    \n","    if os.path.exists(case_dir):\n","        # Walk through all files in the series directories within the \"Unknown Study\" directory\n","        for root, dirs, files in os.walk(case_dir):\n","            for dir in dirs:\n","                series_path = os.path.join(root, dir)\n","                slices = []\n","                # Collect all DICOM slices in the series directory\n","                for slice_file in os.listdir(series_path):\n","                    if slice_file.lower().endswith('.dcm'):\n","                        full_path = os.path.join(series_path, slice_file)\n","                        try:\n","                            # Read the DICOM file\n","                            dicom_slice = pydicom.dcmread(full_path)\n","                            # Append the slice and its position for later sorting\n","                            slices.append((dicom_slice, dicom_slice.ImagePositionPatient[2]))\n","                            dicom_file_count += 1\n","                        except Exception as e:\n","                            print(f\"Failed to read {slice_file} as DICOM: {e}\")\n","\n","                # Sort slices based on the z-coordinate (ImagePositionPatient[2])\n","                slices.sort(key=lambda x: x[1])\n","                # Stack the pixel data from sorted slices to form a 3D volume\n","                if slices:\n","                    series_uid = slices[0][0].SeriesInstanceUID\n","                    volume = np.stack([s[0].pixel_array for s in slices])\n","\n","                    # Resize the volume\n","                    resize_factor = np.array(target_size) / np.array(volume.shape)\n","                    resized_volume = zoom(volume, resize_factor)\n","                    series_volumes[series_uid].append(resized_volume)\n","\n","    else:\n","        print(f\"Directory does not exist: {case_dir}\")\n","\n","    print(f\"Processed {dicom_file_count} DICOM files.\")\n","    return series_volumes\n","\n","# Usage example\n","base_directory = '/media/hal9000/Database/CQ500_extracted'  # Adjust this path\n","dicom_volumes = load_dicom_series_volumes(base_directory, 200)\n","print(f\"Number of series with loaded volumes: {len(dicom_volumes)}\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Initialize the mask data dictionary\n","mask_data = {}"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data for AE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for AE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for AE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for AE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for FBP from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for FBP from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for FBP from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for FBP from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for GT from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for GT from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for GT from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for GT from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for Noisy from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for Noisy from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for Noisy from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for Noisy from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for VAE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for VAE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for VAE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for VAE from CT1.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714313.2033589\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n"]}],"source":["# Load data from NII files (Brain0 clipped data)\n","# load nii files from Will's folder, update mask_dir and brain_dir to update lists.\n","\n","# Define the directories\n","brain_dir = './Noise Data/Brain0'\n","mask_dir = '/media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0'\n","\n","# Define the target shape\n","target_shape = (32, 128, 128)\n","\n","# Initialize the dictionary\n","brain_data = {\"AE\": {}, \"FBP\": {}, \"GT\": {}, \"Noisy\": {}, \"VAE\": {}}\n","\n","# Define the correspondence between the file names and the series_uids\n","folder_to_series_uid = {\n","    \"CTD3D\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\",\n","    \"CTD3D2\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\",\n","    \"CTD3D3\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\",\n","    \"CTP\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\"\n","}\n","\n","# Load the brain data\n","for key in brain_data.keys():\n","    subdir_path = os.path.join(brain_dir, key)\n","    if os.path.isdir(subdir_path):\n","        for root, dirs, files in os.walk(subdir_path):\n","            for dir in dirs:\n","                dir_path = os.path.join(root, dir)\n","                for file in os.listdir(dir_path):\n","                    if file.endswith('.nii'):\n","                        img = nib.load(os.path.join(dir_path, file))\n","                        data = img.get_fdata()\n","                        resized_data = zoom(data, (target_shape[0]/data.shape[0], target_shape[1]/data.shape[1], target_shape[2]/data.shape[2]))\n","                        series_uid = folder_to_series_uid.get(dir)  # Get the series_uid from the dictionary using the directory name\n","                        if series_uid:  # If the series_uid exists\n","                            brain_data[key][series_uid] = resized_data  # Store the resized data in the dictionary with the series_uid as the key\n","                            print(f\"Loaded data for {key} from {file} and associated with {series_uid}\")\n","\n","\n","# Load the mask data\n","for root, dirs, files in os.walk(mask_dir):\n","    for file in files:\n","        if file.endswith('.nii'):\n","            series_uid = file.split('_')[0]  # Extract the series_uid from the file name\n","            img = nib.load(os.path.join(root, file))\n","            data = img.get_fdata()\n","            resized_data = zoom(data, (target_shape[0]/data.shape[0], target_shape[1]/data.shape[1], target_shape[2]/data.shape[2]))\n","            mask_data[series_uid] = resized_data  # Store the resized data in the dictionary with the series_uid as the key\n","            print(f\"Loaded mask data for {series_uid}\")\n","\n","# Initialize the dictionary\n","data_dict = {\"AE\": {}, \"FBP\": {}, \"GT\": {}, \"Noisy\": {}, \"VAE\": {}}\n","\n","# Iterate over the brain data and mask data to create the tuples\n","for key in data_dict.keys():\n","    for series_uid, brain in brain_data[key].items():\n","        mask = mask_data.get(series_uid)\n","        if mask is not None:\n","            data_dict[key][series_uid] = (brain, mask, None)  # Add a placeholder for the predicted mask values\n"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data for AE from CT 4cc sec 150cc D3D on-2.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for AE from CT 4cc sec 150cc D3D on-3.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for AE from CT 4cc sec 150cc D3D on.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for AE from CT Plain.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for FBP from CT 4cc sec 150cc D3D on-2.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for FBP from CT 4cc sec 150cc D3D on-3.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for FBP from CT 4cc sec 150cc D3D on.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for FBP from CT Plain.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for GT from CT 4cc sec 150cc D3D on-2.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for GT from CT 4cc sec 150cc D3D on-3.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for GT from CT 4cc sec 150cc D3D on.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for GT from CT Plain.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for Noisy from CT 4cc sec 150cc D3D on-2.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for Noisy from CT 4cc sec 150cc D3D on-3.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for Noisy from CT 4cc sec 150cc D3D on.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for Noisy from CT Plain.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded data for VAE from CT 4cc sec 150cc D3D on-2.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded data for VAE from CT 4cc sec 150cc D3D on-3.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded data for VAE from CT 4cc sec 150cc D3D on.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded data for VAE from CT Plain.nii and associated with 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714313.2033589\n","Loaded mask data for 1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\n"]}],"source":["# Load data from NII files (new expanded version)\n","# load nii files from Will's folder, update mask_dir and brain_dir to update lists.\n","\n","# Define the directories\n","brain_dir = './Noise Data/B0_expanded'\n","mask_dir = '/media/hal9000/Database/CQ500_extracted/CQ500CT0 CQ500CT0'\n","\n","# Define the target shape\n","target_shape = (32, 128, 128)\n","\n","# Initialize the dictionary\n","brain_data = {\"AE\": {}, \"FBP\": {}, \"GT\": {}, \"Noisy\": {}, \"VAE\": {}}\n","\n","# Define the correspondence between the file names and the series_uids\n","file_to_series_uid = {\n","    \"CT 4cc sec 150cc D3D on.nii\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\",\n","    \"CT 4cc sec 150cc D3D on-2.nii\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\",\n","    \"CT 4cc sec 150cc D3D on-3.nii\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\",\n","    \"CT Plain.nii\": \"1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075\"\n","}\n","\n","# Load the brain data\n","for key in brain_data.keys():\n","    subdir_path = os.path.join(brain_dir, key)\n","    if os.path.isdir(subdir_path):\n","        for root, dirs, files in os.walk(subdir_path):\n","            for file in sorted(files):\n","                if file.endswith('.nii'):\n","                    img = nib.load(os.path.join(root, file))\n","                    data = img.get_fdata()\n","                    resized_data = zoom(data, (target_shape[0]/data.shape[0], target_shape[1]/data.shape[1], target_shape[2]/data.shape[2]))\n","                    series_uid = file_to_series_uid.get(file)  # Get the series_uid from the dictionary\n","                    if series_uid:  # If the series_uid exists\n","                        brain_data[key][series_uid] = resized_data  # Store the resized data in the dictionary with the series_uid as the key\n","                        print(f\"Loaded data for {key} from {file} and associated with {series_uid}\")\n","\n","\n","# Load the mask data\n","for root, dirs, files in os.walk(mask_dir):\n","    for file in files:\n","        if file.endswith('.nii'):\n","            series_uid = file.split('_')[0]  # Extract the series_uid from the file name\n","            img = nib.load(os.path.join(root, file))\n","            data = img.get_fdata()\n","            resized_data = zoom(data, (target_shape[0]/data.shape[0], target_shape[1]/data.shape[1], target_shape[2]/data.shape[2]))\n","            mask_data[series_uid] = resized_data  # Store the resized data in the dictionary with the series_uid as the key\n","            print(f\"Loaded mask data for {series_uid}\")\n","\n","# Initialize the dictionary\n","data_dict = {\"AE\": {}, \"FBP\": {}, \"GT\": {}, \"Noisy\": {}, \"VAE\": {}}\n","\n","# Iterate over the brain data and mask data to create the tuples\n","for key in data_dict.keys():\n","    for series_uid, brain in brain_data[key].items():\n","        mask = mask_data.get(series_uid)\n","        if mask is not None:\n","            data_dict[key][series_uid] = (brain, mask, None)  # Add a placeholder for the predicted mask values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CTD3D     1.2.276.0.7230010.3.1.3.296485376.1.1521714308.2032152\n","# CTD3D2    1.2.276.0.7230010.3.1.3.296485376.1.1521714311.2033110\n","# CTD3D3    1.2.276.0.7230010.3.1.3.296485376.1.1521714309.2032631\n","# CTP       1.2.276.0.7230010.3.1.3.296485376.1.1521714314.2034075"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["def dice_coef(pred, target, smooth = 1.):\n","    \"\"\"\n","    This function calculates the Dice loss between two binary masks.\n","\n","    Parameters:\n","    - pred: The predicted mask.\n","    - target: The ground truth mask.\n","    - smooth: A smoothing factor to avoid division by zero.\n","\n","    Returns:\n","    - The Dice loss between the predicted and ground truth masks.\n","    \"\"\"\n","    pred = pred.contiguous()\n","    target = target.contiguous()\n","\n","    intersection = (pred * target).sum(dim=2).sum(dim=2)\n","    \n","    loss = ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n","    \n","    return loss.mean()"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[],"source":["def visualize_all_masks(image_volume, mask_volume, ae_volume, ae_mask_volume, fbp_volume, fbp_mask_volume, gt_volume, gt_mask_volume, noisy_volume, noisy_mask_volume, vae_volume, vae_mask_volume):\n","    # Create a colormap that includes transparency\n","    colors = [(0,0,0,0)] + [(plt.cm.Reds(i)) for i in range(1,256)]\n","    RedAlpha = mcolors.LinearSegmentedColormap.from_list('RedAlpha', colors, N=256)\n","\n","    colors = [(0,0,0,0)] + [(plt.cm.Blues(i)) for i in range(1,256)]\n","    BlueAlpha = mcolors.LinearSegmentedColormap.from_list('BlueAlpha', colors, N=256)\n","\n","    # Function to update the plot for each slice\n","    def plot_slice(slice_index):\n","        fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n","        \n","        # Show the ground truth mask slice\n","        axs[0].imshow(image_volume[slice_index], cmap='gray')\n","        axs[0].imshow(mask_volume[slice_index], alpha=0.75, cmap=RedAlpha)  # Overlay mask\n","        axs[0].set_title('Ground Truth')\n","        axs[0].axis('off')\n","\n","        # Show the FBP mask slice\n","        axs[1].imshow(ae_volume[slice_index], cmap='gray')\n","        axs[1].imshow(ae_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[1].set_title(f'AE Pred Mask\\nDice: {dice_coef(torch.from_numpy(ae_mask_volume).unsqueeze(0).unsqueeze(0), torch.from_numpy(mask_volume).unsqueeze(0).unsqueeze(0)).item():.4f}')\n","        axs[1].axis('off')\n","\n","        # Show the Noisy mask slice\n","        axs[2].imshow(fbp_volume[slice_index], cmap='gray')\n","        axs[2].imshow(fbp_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[2].set_title(f'FBP Pred Mask\\nDice: {dice_coef(torch.from_numpy(fbp_mask_volume).unsqueeze(0).unsqueeze(0), torch.from_numpy(mask_volume).unsqueeze(0).unsqueeze(0)).item():.4f}')\n","        axs[2].axis('off')\n","\n","        # Show the AE mask slice\n","        axs[3].imshow(gt_volume[slice_index], cmap='gray')\n","        axs[3].imshow(gt_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[3].set_title(f'GT Pred Mask\\nDice: {dice_coef(torch.from_numpy(gt_mask_volume).unsqueeze(0).unsqueeze(0), torch.from_numpy(mask_volume).unsqueeze(0).unsqueeze(0)).item():.4f}')\n","        axs[3].axis('off')\n","\n","        # Show the VAE mask slice\n","        axs[4].imshow(noisy_volume[slice_index], cmap='gray')\n","        axs[4].imshow(noisy_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[4].set_title(f'Noisy Pred Mask\\nDice: {dice_coef(torch.from_numpy(noisy_mask_volume).unsqueeze(0).unsqueeze(0), torch.from_numpy(mask_volume).unsqueeze(0).unsqueeze(0)).item():.4f}')\n","        axs[4].axis('off')\n","\n","        # Show the actual ground truth mask slice\n","        axs[5].imshow(vae_volume[slice_index], cmap='gray')\n","        axs[5].imshow(vae_mask_volume[slice_index], alpha=0.75, cmap=BlueAlpha)  # Overlay mask\n","        axs[5].set_title(f'VAE Pred Mask\\nDice: {dice_coef(torch.from_numpy(vae_mask_volume).unsqueeze(0).unsqueeze(0), torch.from_numpy(mask_volume).unsqueeze(0).unsqueeze(0)).item():.4f}')\n","        axs[5].axis('off')\n","\n","        plt.show()\n","\n","    # Create a slider to scroll through slices\n","    interact(plot_slice, slice_index=IntSlider(min=0, max=image_volume.shape[0] - 1, step=1, value=image_volume.shape[0] // 2))\n","\n","# Call the function with the appropriate mask volumes\n"]},{"cell_type":"code","execution_count":217,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8ccc2986b9548fa873d136d46507a1b","version_major":2,"version_minor":0},"text/plain":["interactive(children=(IntSlider(value=16, description='slice_index', max=31), Output()), _dom_classes=('widget…"]},"metadata":{},"output_type":"display_data"}],"source":["# Test on unseen data\n","directories = ['AE', 'FBP', 'GT', 'Noisy', 'VAE']\n","pred_volume_mask = []\n","series_selection =  3\n","\n","# Loop over all directories\n","for directory in directories:\n","\n","    series_uid = list(data_dict[directory].keys())[series_selection]\n","    volume, ground_truth_mask, _ = data_dict[directory][series_uid]  # Get the volume and ground truth mask of the first series\n","\n","    ground_truth_mask = ground_truth_mask > 0.5\n","\n","    model.eval()  # Set the model to evaluation mode\n","    with torch.no_grad():  # Disable gradient calculation\n","        images = torch.from_numpy(volume.astype(np.float32))\n","        # Add a channel dimension (assuming grayscale images)\n","        images = images.unsqueeze(0)\n","        # Add a batch dimension\n","        images = images.unsqueeze(0)\n","        # Ensure the tensor is on the correct device\n","        images = images.to(device)\n","        outputs = model(images)  # Get the model's predictions\n","\n","    pred_volume_mask.append((outputs > 0).float().squeeze(0)[0].cpu().numpy())\n","\n","    # Update the predicted mask in the data dictionary\n","    data_dict[directory][series_uid] = (volume, ground_truth_mask, pred_volume_mask)\n","\n","ae_vol, _, _ = data_dict['AE'][list(data_dict[directory].keys())[series_selection]]\n","fbp_vol, _, _ = data_dict['FBP'][list(data_dict[directory].keys())[series_selection]]\n","gt_vol, _, _ = data_dict['GT'][list(data_dict[directory].keys())[series_selection]]\n","noisy_vol, _, _ = data_dict['Noisy'][list(data_dict[directory].keys())[series_selection]]\n","vae_vol, _, _ = data_dict['VAE'][list(data_dict[directory].keys())[series_selection]]\n","\n","\n","# Visualize the results\n","visualize_all_masks(volume, ground_truth_mask, ae_vol, pred_volume_mask[0], fbp_vol, pred_volume_mask[1], gt_vol, pred_volume_mask[2], noisy_vol, pred_volume_mask[3], vae_vol, pred_volume_mask[4])"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["# Visualize a 3D mesh from the predicted volume mask\n","bool_pred_volume_mask = (pred_volume_mask > 0).astype(np.float32)\n","\n","zoom_factor = (4, 1, 1)\n","resized_pred_volume_mask = zoom(bool_pred_volume_mask, zoom_factor)[30:-30, 30:-30, 30:-30]\n","\n","rotated_pred_volume_mask = np.transpose(resized_pred_volume_mask, (2, 1, 0))\n","\n","# Visualize the CSF Space Segmentation Mask in 3D dynamic viewer\n","def visualize_3d_mask(volume_mask):\n","    \"\"\"Visualize a 3D mask using matplotlib's 3D capabilities.\"\"\"\n","    # Apply a Gaussian filter to the volume mask to make the surface smoother\n","    smoothed_volume_mask = gaussian_filter(volume_mask, sigma=1)\n","    print(np.unique(smoothed_volume_mask))\n","\n","    # Generate a 3D mesh from the smoothed volume mask\n","    verts, faces, _, _ = measure.marching_cubes(smoothed_volume_mask)\n","\n","    # Create a new figure and add a 3D subplot\n","    fig = plt.figure(figsize=(10, 10))\n","    ax = fig.add_subplot(111, projection='3d')\n","\n","    # Create a 3D polygon collection\n","    mesh = Poly3DCollection(verts[faces], alpha=0.4, edgecolor='r')\n","    ax.add_collection3d(mesh)\n","\n","    # Set the limits of the plot to the limits of the data\n","    max_range = np.array([volume_mask.shape[0]-1, volume_mask.shape[1]-1, volume_mask.shape[2]-1]).max() / 2.0\n","\n","    mid_x = (volume_mask.shape[0]) / 2.0\n","    mid_y = (volume_mask.shape[1]) / 2.0\n","    mid_z = (volume_mask.shape[2]) / 2.0\n","\n","    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n","    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n","    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n","\n","    # Rotate the view\n","    ax.view_init(azim=-30)\n","\n","    # Show the plot\n","    plt.show()\n","\n","# Assuming volume_mask is the mask calculated earlier\n","visualize_3d_mask(rotated_pred_volume_mask)"]},{"cell_type":"code","execution_count":218,"metadata":{},"outputs":[],"source":["# Save model\n","#naming is model{num_images}e{num_epochs}.pth\n","\n","PATH = \"./Models/model100e100_c_n.pth\"\n","torch.save(model.state_dict(), PATH)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for UNet3D:\n\tsize mismatch for up1.conv.double_conv.0.weight: copying a param with shape torch.Size([384, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for up1.conv.double_conv.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.3.weight: copying a param with shape torch.Size([256, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.0.weight: copying a param with shape torch.Size([192, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.running_mean: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.running_var: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.3.weight: copying a param with shape torch.Size([128, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.0.weight: copying a param with shape torch.Size([96, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.0.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.3.weight: copying a param with shape torch.Size([64, 96, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Models/model100e300.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet3D(n_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet3D:\n\tsize mismatch for up1.conv.double_conv.0.weight: copying a param with shape torch.Size([384, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3, 3]).\n\tsize mismatch for up1.conv.double_conv.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.running_mean: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.1.running_var: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up1.conv.double_conv.3.weight: copying a param with shape torch.Size([256, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.0.weight: copying a param with shape torch.Size([192, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3, 3]).\n\tsize mismatch for up2.conv.double_conv.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.running_mean: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.1.running_var: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up2.conv.double_conv.3.weight: copying a param with shape torch.Size([128, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.0.weight: copying a param with shape torch.Size([96, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3, 3]).\n\tsize mismatch for up3.conv.double_conv.0.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.weight: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.running_mean: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.1.running_var: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for up3.conv.double_conv.3.weight: copying a param with shape torch.Size([64, 96, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3])."]}],"source":["# Load model\n","PATH = \"./Models/model100e300.pth\"\n","\n","model = UNet3D(n_channels=1, n_classes=1).to(device)\n","model.load_state_dict(torch.load(PATH))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
